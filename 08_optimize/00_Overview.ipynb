{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize Models using Automatic Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker automatic model tuning, also called hyperparameter tuning (HPT), finds the best performing model against a defined objective metric by running many training jobs on the dataset using the algorithm and ranges of hyperparameters that you specify. SageMaker HPT supports both random search and bayesian optimization as tuning strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/hpt.png\" width=\"90%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Warm Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warm start configuration allows you to create a new tuning job with the learning gathered in a parent tuning job by specifying up to 5 parent tuning jobs. If a warm start configuration is specified, Automatic Model Tuning will load the previous [hyperparameter set, objective metrics values] to warm start the new tuning job. This means, you can continue optimizing your model from the point you finished your previous tuning job experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/hpt-warmstart.png\" width=\"90%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warm start is particularly useful in a couple of scenarios. \n",
    "\n",
    "1) You might want to change the hyperparameter tuning ranges from the previous job by adding more hyperparameters to tune or the change the search range for some hyperparameters. \n",
    "\n",
    "2) You might have collected more training data in the meantime and want to re-run the tuning job with the updated dataset. Both scenarios benefit from the knowledge of the previous tuning job to find the best model faster. \n",
    "\n",
    "The two scenarios are implemented with two warm start types, `IDENTICAL_DATA_AND_ALGORITHM` and `TRANSFER_LEARNING`. \n",
    "\n",
    "If you choose `IDENTICAL_DATA_AND_ALGORITHM`, the new HPT job uses the same input data and training image as the parent job. You are allowed to update the hyperparameter tuning ranges and the maximum number of training jobs. You can also add previously static hyperparameters to the list of tunable hyperparameters and vice versa, as long as the overall number of static plus tunable hyperparameters remains the same. Upon completion, a tuning job with this strategy will return an additional field, OverallBestTrainingJob containing the best model candidate including this tuning job as well as the completed parent tuning jobs.\n",
    "\n",
    "\n",
    "If you choose `TRANSFER_LEARNING`, you can additionally use updated training data, and a different version of the training algorithms image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.save_checkpoint();\n",
    "Jupyter.notebook.session.delete();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
