{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Factorization Machine in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a sparse matrix which is needed for feeding in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import csr\n",
    "\n",
    "def vectorize_dic(dic, ix=None, p=None):\n",
    "    \"\"\" \n",
    "    Creates a scipy csr matrix from a list of lists (each inner list is a set of values corresponding to a feature) \n",
    "    \n",
    "    parameters:\n",
    "    -----------\n",
    "    dic -- dictionary of feature lists. Keys are the name of features\n",
    "    ix -- index generator (default None)\n",
    "    p -- dimension of featrure space (number of columns in the sparse matrix) (default None)\n",
    "    \"\"\"\n",
    "    if (ix == None):\n",
    "        ix = defaultdict(count(0).__next__)\n",
    "        \n",
    "    n = len(dic.values()[0]) # num samples\n",
    "    g = len(dic.keys()) # num groups\n",
    "    nz = n * g # number of non-zeros\n",
    "\n",
    "    col_ix = np.empty(nz, dtype=int)\n",
    "    \n",
    "    i = 0\n",
    "    for k, lis in dic.iteritems():\n",
    "        # append index el with k in order to prevet mapping different columns with same id to same index\n",
    "        col_ix[i::g] = [ix[str(el) + str(k)] for el in lis]\n",
    "        i += 1\n",
    "        \n",
    "    row_ix = np.repeat(np.arange(0, n), g)\n",
    "    data = np.ones(nz)\n",
    "    \n",
    "    if (p == None):\n",
    "        p = len(ix)\n",
    "        \n",
    "    ixx = np.where(col_ix < p)\n",
    "\n",
    "    return csr.csr_matrix((data[ixx],(row_ix[ixx], col_ix[ixx])), shape=(n, p)), ix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reacing in data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_ratings = pd.read_csv(\"NLP_cleaned_ratings.csv\", low_memory=False)\n",
    "df_items = pd.read_csv(\"NLP_cleaned_items.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_ratings.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limit row for first run"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_ratings = df_ratings[['user_id', \"item_id\", \"rating\", \"timestamp\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Train Split"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "main = np.random.rand(len(df_ratings)) < 0.7\n",
    "\n",
    "train = df_ratings[main]\n",
    "\n",
    "test = df_ratings[~main]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(\"\\nmetadata dimensions\")\n",
    "print(train.shape)\n",
    "print(\"\\nrating dimensions\")\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# vectorize data and convert them to csr matrix\n",
    "X_train, ix = vectorize_dic({'users': train.user_id.values, 'items': train.item_id.values})\n",
    "X_test, ix = vectorize_dic({'users': test.user_id.values, 'items': test.item_id.values}, ix, X_train.shape[1])\n",
    "y_train = train.rating.values\n",
    "y_test= test.rating.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.loadtxt('fm_Xtrain.txt')\n",
    "X_test = np.loadtxt('fm_Xtest.txt')\n",
    "y_train = np.loadtxt('fm_ytrain.txt')\n",
    "y_test = np.loadtxt('fm_ytest.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape\n",
    "# X_train = train\n",
    "# print X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "n, p = X_train.shape\n",
    "\n",
    "# number of latent factors\n",
    "k = 100\n",
    "\n",
    "# design matrix\n",
    "X = tf.placeholder('float', shape=[None, p])\n",
    "# target vector\n",
    "y = tf.placeholder('float', shape=[None, 1])\n",
    "\n",
    "# bias and weights\n",
    "w0 = tf.Variable(tf.zeros([1]))\n",
    "W = tf.Variable(tf.zeros([p]))\n",
    "\n",
    "# interaction factors, randomly initialized \n",
    "V = tf.Variable(tf.random_normal([k, p], stddev=0.01))\n",
    "\n",
    "# estimate of y, initialized to 0.\n",
    "y_hat = tf.Variable(tf.zeros([n, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate output with FM equation\n",
    "linear_terms = tf.add(w0, tf.reduce_sum(tf.multiply(W, X), 1, keep_dims=True))\n",
    "pair_interactions = (tf.multiply(0.5,\n",
    "                    tf.reduce_sum(\n",
    "                        tf.subtract(\n",
    "                            tf.pow( tf.matmul(X, tf.transpose(V)), 2),\n",
    "                            tf.matmul(tf.pow(X, 2), tf.transpose(tf.pow(V, 2)))),\n",
    "                        1, keep_dims=True)))\n",
    "y_hat = tf.add(linear_terms, pair_interactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# L2 regularized sum of squares loss function over W and V\n",
    "lambda_w = tf.constant(0.001, name='lambda_w')\n",
    "lambda_v = tf.constant(0.001, name='lambda_v')\n",
    "\n",
    "l2_norm = (tf.reduce_sum(\n",
    "            tf.add(\n",
    "                tf.multiply(lambda_w, tf.pow(W, 2)),\n",
    "                tf.multiply(lambda_v, tf.pow(V, 2)))))\n",
    "\n",
    "error = tf.reduce_mean(tf.square(tf.subtract(y, y_hat)))\n",
    "loss = tf.add(error, l2_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batcher(X_, y_=None, batch_size=-1):\n",
    "    n_samples = X_.shape[0]\n",
    "\n",
    "    if batch_size == -1:\n",
    "        batch_size = n_samples\n",
    "    if batch_size < 1:\n",
    "       raise ValueError('Parameter batch_size={} is unsupported'.format(batch_size))\n",
    "\n",
    "    for i in range(0, n_samples, batch_size):\n",
    "        upper_bound = min(i + batch_size, n_samples)\n",
    "        ret_x = X_[i:upper_bound]\n",
    "        ret_y = None\n",
    "        if y_ is not None:\n",
    "            ret_y = y_[i:i + batch_size]\n",
    "            yield (ret_x, ret_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54211350f1604124981410467bb889d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Total Run Time\n",
      "66.91984987258911 seconds\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "import time\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 1000\n",
    "\n",
    "# Launch the graph\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "\n",
    "start_time_total = time.time()\n",
    "sess.run(init)\n",
    "\n",
    "for epoch in tqdm(range(epochs), unit='epoch'):\n",
    "    perm = np.random.permutation(X_train.shape[0])\n",
    "    # iterate over batches\n",
    "    for bX, bY in batcher(X_train[perm], y_train[perm], batch_size):\n",
    "        sess.run(optimizer, feed_dict={X: bX.reshape(-1, p), y: bY.reshape(-1, 1)})\n",
    "\n",
    "print(\"\\nTotal Run Time\")\n",
    "print(time.time() - start_time_total, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1028684\n"
     ]
    }
   ],
   "source": [
    "errors = []\n",
    "for bX, bY in batcher(X_test, y_test):\n",
    "    errors.append(sess.run(error, feed_dict={X: bX.reshape(-1, p), y: bY.reshape(-1, 1)}))\n",
    "\n",
    "RMSE = np.sqrt(np.array(errors).mean())\n",
    "print(RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
