{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Data with AWS Data Wrangler\n",
    "* https://github.com/awslabs/aws-data-wrangler\n",
    "* https://aws-data-wrangler.readthedocs.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWS Data Wrangler is an open-source Python package that extends the power of the Pandas library to AWS connecting DataFrames and AWS data related services (Amazon Redshift, AWS Glue, Amazon Athena, Amazon EMR, Amazon QuickSight, etc).\n",
    "\n",
    "Built on top of other open-source projects like Pandas, Apache Arrow, Boto3, s3fs, SQLAlchemy, Psycopg2 and PyMySQL, it offers abstracted functions to execute usual ETL tasks like load/unload data from Data Lakes, Data Warehouses and Databases.\n",
    "\n",
    "_Note that AWS Data Wrangler is simply a Python library that uses existing AWS Services.  AWS Data Wrangler is not a separate AWS Service.  You install AWS Data Wrangler through `pip install` as we will see next._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q awswrangler==1.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "# Get region \n",
    "session = boto3.session.Session()\n",
    "region_name = session.region_name\n",
    "\n",
    "# Get SageMaker session & default S3 bucket\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import awswrangler as wr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Parquet from S3 with Push-Down Filters\n",
    "\n",
    "Read Apache Parquet file(s) from from a received S3 prefix or list of S3 objects paths.\n",
    "\n",
    "The concept of Dataset goes beyond the simple idea of files and enable more complex features like partitioning and catalog integration (AWS Glue Catalog): \n",
    "\n",
    "dataset (bool) â€“ If True read a parquet dataset instead of simple file(s) loading all the related partitions as columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '{}/amazon-reviews-pds/parquet/'.format(bucket)\n",
    "df_parquet_results = wr.s3.read_parquet(path,\n",
    "                                     columns=['star_rating', 'product_category', 'review_body'],\n",
    "                                     filters=[(\"product_category\", \"=\", \"Digital_Software\")],\n",
    "                                     dataset=True)\n",
    "df_parquet_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet_results.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Parquet from S3 in Chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batching (chunked argument) (Memory Friendly):\n",
    "\n",
    "Will enable the function to return a Iterable of DataFrames instead of a regular DataFrame.\n",
    "\n",
    "There are two batching strategies on Wrangler:\n",
    "* If chunked=True, a new DataFrame will be returned for each file in your path/dataset.\n",
    "* If chunked=INTEGER, Wrangler will iterate on the data by number of rows equal to the received INTEGER.\n",
    "\n",
    "P.S. chunked=True if faster and uses less memory while chunked=INTEGER is more precise in number of rows for each Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '{}/amazon-reviews-pds/parquet/'.format(bucket)\n",
    "chunk_iter = wr.s3.read_parquet(path,\n",
    "                                columns=['star_rating', 'product_category', 'review_body'],\n",
    "                                filters=[(\"product_category\", \"=\", \"Digital_Software\")],\n",
    "                                dataset=True,\n",
    "                                chunked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(next(chunk_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query the Glue Catalog (ie. Hive Metastore)\n",
    "Get an iterator of tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_name = 'dsoaws'\n",
    "table_name_tsv = 'amazon_reviews_tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table in wr.catalog.get_tables(database=\"dsoaws\"):\n",
    "    print(table['Name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query from Athena\n",
    "Execute any SQL query on AWS Athena and return the results as a Pandas DataFrame.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = wr.athena.read_sql_query(\n",
    "    sql='SELECT * FROM {} LIMIT 5000'.format(table_name_tsv),\n",
    "    database=database_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query from Athena in Chunks\n",
    "Retrieving in chunks can help reduce memory requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "chunk_iter = wr.athena.read_sql_query(\n",
    "    sql='SELECT * FROM {} LIMIT 5000'.format(table_name_tsv),\n",
    "    database='{}'.format(database_name),\n",
    "    chunksize=64_000  # 64 KB Chunks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(chunk_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.save_checkpoint();\n",
    "Jupyter.notebook.session.delete();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
